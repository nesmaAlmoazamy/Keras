#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Sep  2 16:29:39 2018

@author: nesma
"""

from __future__ import print_function
#Import Dataset, neural network
import keras
from keras.datasets import mnist
from keras.models import Sequential, load_model, optimizers
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
#Import for handling arrays
import numpy as np
import os
#Import for Plotting
import matplotlib.pyplot as plt
import psutil
import time
import datetime



os.environ['TF_CPP_MIN_LOG_LEVEL']='3'
#from keras import backend as K

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

#Load the dataset
#Split the dataset into train set and test set
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# building the input vector from the 28x28 pixels
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')



input_shape = (img_rows, img_cols, 1)

# normalizing the data to help with the training
x_train /= 255
x_test /= 255

# print the input shape
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
#The result is a vector with a length equal to the number of categories.
print("Shape before encoding: ", y_train.shape)
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print("Shape after encoding: ", y_train.shape)

print("1")
# building model layers with the sequential model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

#For the multi-class classification 
#we add another densely-connected (or fully-connected) layer 
#for the 10 different output classes
#Activation function for multiclass targets

model.add(Dense(num_classes, activation='softmax'))
print("2")

# compiling the sequential model
#metrics function is close to the loss function
#
#model.compile(loss=keras.losses.categorical_crossentropy,
#              optimizer='adam',
#              metrics=['accuracy'])



sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)


sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

print("3")

#Training the model

#the bigger the batch, the more stable our stochastic gradient descent updates will be.
# training the model and saving metrics in CNNModel



process = psutil.Process(os.getpid())
print("before memory_percent",process.memory_percent())
print(psutil.cpu_percent(percpu=True))

start = time.time()
print("start",start)
CNNModel = model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
end = time.time()

process = psutil.Process(os.getpid())
print("after memory_percent",process.memory_percent())
print(psutil.cpu_percent(percpu=True))


print("end", end)

print("Time Elapsed")
print(str(datetime.timedelta(seconds= end - start)))
print(end - start)


save_dir = "/home/nesma/results/"
model_name = 'mnist_keras_cnn_model.h5'
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print('Saved trained model at %s ' % model_path)

print("4")

#Model performance evaluation
Test_loss , acc = model.evaluate(x_test, y_test, verbose=1)
print("Test Loss", Test_loss)
print("Test Accuracy", acc)

model.save('mnist_keras_cnn_model.h5')

# 
# 
# # load the model and create predictions on the test set
#mnist_model = load_model(model_path)
#predicted_classes = mnist_model.predict_classes(x_test)
# 
# # see which we predicted correctly and which not
#correct_indices = np.nonzero(predicted_classes == y_test)[0]
#incorrect_indices = np.nonzero(predicted_classes != y_test)[0]
#
#print(len(correct_indices)," classified correctly")
#print(len(incorrect_indices)," classified incorrectly")
# 
# # adapt figure size to accomodate 18 subplots
#plt.rcParams['figure.figsize'] = (7,14)
# 
#figure_evaluation = plt.figure()
# 
# # plot 9 correct predictions
#for i, correct in enumerate(correct_indices[:9]):
#     plt.subplot(6,3,i+1)
#     plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')
#     plt.title(
#       "Predicted: {}, Truth: {}".format(predicted_classes[correct],
#                                         y_test[correct]))
#     plt.xticks([])
#     plt.yticks([])
# 
# # plot 9 incorrect predictions
#for i, incorrect in enumerate(incorrect_indices[:9]):
#     plt.subplot(6,3,i+10)
#     plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
#     plt.title(
#       "Predicted {}, Truth: {}".format(predicted_classes[incorrect], 
#                                        y_test[incorrect]))
#     plt.xticks([])
#     plt.yticks([])
# 
#figure_evaluation
#
#
